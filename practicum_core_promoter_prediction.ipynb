{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xl52C6-RJKt0"
      },
      "outputs": [],
      "source": [
        "import warnings;\n",
        "warnings.filterwarnings('ignore');\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO24y4Jju21u"
      },
      "source": [
        "# **Core promoter prediction from DNA sequence**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhcU9Uucu21v"
      },
      "source": [
        "Promoter detection in the human genome focuses on identifying proximal promoter regions, crucial sequences responsible for initiating transcription. Accurate identification of these sites is essential for advancing our understanding of gene regulation mechanisms and identifying the genomic basis of numerous diseases. This assignment involves training and evaluating a core promoter prediction model, which aims to predict the central core promoter region closest to the transcription start site (TSS) and start codon.\n",
        "\n",
        "Promoter sequences are extracted as -34 to +35 base pairs around the TSS from TATA promoters, sourced from the Eukaryotic Promoter Database (EPDnew) [1]. The non-promoter class is constructed using equal-sized randomly selected sequences outside promoter regions that contain the TATA motif (TATA non-promoters).\n",
        "\n",
        "The goal of this assignment is to develop a model that accurately distinguishes core promoter regions from non-promoter regions, thereby contributing to our understanding of the fundamental elements governing gene expression.\n",
        "\n",
        "First we load the train and test data:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xrdBiQtAu21v"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sequence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ACTACAAATGGACGAGAGAGGCGGCCGTCCATTAGTTAGCGGCTCC...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AAAATATAGGCCGGGGTACCTCAGCCGGAAGGGACTTCAGTTAGTG...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CCTACATAAGTCCCTGTATAAAGTCACTGACCCATTTGCACTGCTG...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AGTTTAAAAGCCAGCCAGTCATACTAAAAAAAAGAATTCAGGTTTT...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>GGGATAAGAAGGACAGAGAGAGACTGTAGGAAGTCAAGGGGTGGAG...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sequence  label\n",
              "0  ACTACAAATGGACGAGAGAGGCGGCCGTCCATTAGTTAGCGGCTCC...      1\n",
              "1  AAAATATAGGCCGGGGTACCTCAGCCGGAAGGGACTTCAGTTAGTG...      1\n",
              "2  CCTACATAAGTCCCTGTATAAAGTCACTGACCCATTTGCACTGCTG...      1\n",
              "3  AGTTTAAAAGCCAGCCAGTCATACTAAAAAAAAGAATTCAGGTTTT...      0\n",
              "4  GGGATAAGAAGGACAGAGAGAGACTGTAGGAAGTCAAGGGGTGGAG...      0"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_train = pd.read_csv(\"https://raw.githubusercontent.com/sdgroeve/practicum_core_promoter_prediction/main/prom_core_tata/train.csv\")\n",
        "data_test = pd.read_csv(\"https://raw.githubusercontent.com/sdgroeve/practicum_core_promoter_prediction/main/prom_core_tata/test.csv\")\n",
        "\n",
        "data_train.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXgIExxtu216"
      },
      "source": [
        "There are just two columns.\n",
        "\n",
        "The column \"sequence\" contains the -34 to +35 base pairs around the candididate TSS.\n",
        "\n",
        "The column \"label\" contains the label. There are two classes: 1 for a core promoter region and 0 for not core promoter region.\n",
        "\n",
        "*Q1: How many sequences does the train set contain for each class?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYhNkKz3u217"
      },
      "outputs": [],
      "source": [
        "###Start code here\n",
        "\n",
        "\n",
        "###End code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4FqVqdmJKt2"
      },
      "source": [
        "### **Feature engineering**\n",
        "\n",
        "Feature engineering is a crucial process in machine learning that involves creating new features or modifying existing ones to improve the performance of a model. It bridges the gap between raw data and the algorithms used for predictive modeling.\n",
        "\n",
        "To derive features from the sequence column, the train and test datasets are initially concatenated into a single Pandas DataFrame. This unified DataFrame enables the application of feature engineering processes uniformly. Subsequently, the original train and test DataFrames can be reconstructed from this consolidated DataFrame.\n",
        "\n",
        "*Q2: Use the Pandas function `concat()` to concatenate the train and test data into a DataFrame called `data`. The train data should be the first rows, with the test data beneath those rows:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kWofh6lJKt2"
      },
      "outputs": [],
      "source": [
        "###Start code here\n",
        "\n",
        "data =\n",
        "\n",
        "###End code here\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5QMBG5wJKt3"
      },
      "source": [
        "*Q3: Pop the `label` column from the `data` DataFrame and assigned it to variable `y`*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4q2YLBWxJKt3"
      },
      "outputs": [],
      "source": [
        "###Start code here\n",
        "\n",
        "y =\n",
        "\n",
        "###End code here\n",
        "\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZkQumNiu22E"
      },
      "source": [
        "Now, it is necessary to represent the local context DNA sequence as a feature vector appropriate for model fitting.\n",
        "\n",
        "Initially, a feature is created for each nucleotide position within the local context DNA sequence.\n",
        "\n",
        "The [pandas.Series.str.split](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.split.html) function splits a string in a column (pandas.Series) from the beginning, at the specified delimiter string:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1T3mn5k6JKt3"
      },
      "outputs": [],
      "source": [
        "data_features = data['sequence'].str.split('', expand=True).iloc[:,1:-1]\n",
        "\n",
        "data_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJhd57jpJKt3"
      },
      "source": [
        "Next, we should assign more meaningful names to the columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8J1BhkRcJKt3"
      },
      "outputs": [],
      "source": [
        "data_features.columns = [\"%i\"%i for i in range(-34,0,1)] + [\"%i\"%i for i in range(0,36,1)]\n",
        "\n",
        "data_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MJXJBj0u22I"
      },
      "source": [
        "The `sklearn.preprocessing.OrdinalEncoder` encode categorical features as an integers.\n",
        "\n",
        "The folloing code creates a Pandas DataFrame `data_features_int_encoding` from `data_features` by applying the `OrdinalEncoder` to map each categorical feature value to an integer. Note the the `.transform()` function in scikit-learn returns a numpy datastructure, wo we need to convert the result back to a Pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXUKcKESJKt4"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "enc = OrdinalEncoder()\n",
        "\n",
        "enc.fit(data_features)\n",
        "\n",
        "data_features_int_encoding = pd.DataFrame(enc.transform(data_features),columns=data_features.columns)\n",
        "\n",
        "data_features_int_encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7r3OK1kJKt5"
      },
      "source": [
        "Finally, we reconstruct the train and test DataFrames based on the number of datapoints in the train data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ogobLcsJKt5"
      },
      "outputs": [],
      "source": [
        "data_features_int_encoding_train = data_features_int_encoding.iloc[:len(data_train),:]\n",
        "data_features_int_encoding_test = data_features_int_encoding.iloc[len(data_train):,:]\n",
        "\n",
        "y_train = y.iloc[:len(data_train)]\n",
        "y_test = y.iloc[len(data_train):]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0573K2rRu22W"
      },
      "source": [
        "### **Model fitting**\n",
        "\n",
        "Cross-validation (CV) is used when evaluating the performance of a machine learning model to ensure it generalizes well to unseen data, by systematically partitioning the data into training and validation sets multiple times. It is particularly useful in cases of limited data, where splitting into separate training and test sets might not be feasible or reliable.\n",
        "\n",
        "*Q4: Estimate the generalization performance of a logisitc regression model with hyperparameter $C=0.1$ on `data_features_int_encoding_train` using 10-fold CV. Use `sklearn.model_selection.cross_val_score` to compute the accuracy for each test fold in the CV. Return the result in `scores`:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc7J8SrIu22X"
      },
      "outputs": [],
      "source": [
        "###Start code here\n",
        "\n",
        "###End code here\n",
        "\n",
        "print(scores.mean())\n",
        "print(scores.std())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9SWqTScu22u"
      },
      "source": [
        "\n",
        "Hyperparameter tuning is the process of optimizing the parameters that govern the learning process of a machine learning model, which are not learned from the data but set prior to training. This process aims to enhance the model's performance by systematically searching through a predefined space of hyperparameters to find the most effective combination.\n",
        "\n",
        "We set hyperparameter $C=1$ for the logistic regression model.\n",
        "\n",
        "*Q5: Use `sklearn.model_selection.GridSearchCV` to find the best performing value for $C$ from the `search_space` search space. Create a GridSearchCV object called `grid_search` and optimize $C$:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hnXUnLMPOcN"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "search_space =  {\n",
        "                \"C\":[0.0001,0.001,0.01,0.1,1,10,100]\n",
        "                }\n",
        "\n",
        "###Start code here\n",
        "\n",
        "###End code here\n",
        "\n",
        "print(grid_search.best_estimator_)\n",
        "print(grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWVs1ZKJTlYv"
      },
      "source": [
        "*Q6: Fit a logistic regression model with the obtained optimal value $C$ on the train set:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHuDj5xsu22v"
      },
      "outputs": [],
      "source": [
        "###Start code here\n",
        "\n",
        "###End code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_gu0seZu221"
      },
      "source": [
        "*Q7: Compute the predictions of this model on the test set. Write these predictions to `predictions`:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzX1UJqlu222"
      },
      "outputs": [],
      "source": [
        "###Start code here\n",
        "\n",
        "###End code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNIux5gFu227"
      },
      "source": [
        "Scikit-learn offers many metrics to evaluate model predictions. These functions are implemented in `sklearn.metrics`.\n",
        "\n",
        "*Q8: Compute the accuracy of the predictions in `predictions`:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTFGB05gu228"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "###Start code here\n",
        "\n",
        "score_acc =\n",
        "\n",
        "###End code here\n",
        "\n",
        "score_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EudhUPu-u23o"
      },
      "source": [
        "### **Feature engineering: one-hot feature encoding**\n",
        "\n",
        "One-hot encoding is a technique used to convert categorical variables into a binary matrix representation where each unique category is represented by a separate column with binary values (0 or 1). It is particularly useful for handling nominal categorical data in machine learning models, ensuring that the model interprets these categories correctly without implying any ordinal relationship.\n",
        "\n",
        "*Q10: Use the Pandas function `get_dummies()` to compute one-hot encoded features. Make sure the feature values are `float`:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7aZedAjJKt6"
      },
      "outputs": [],
      "source": [
        "###Start code here\n",
        "\n",
        "data_features_onehot_encoding =\n",
        "\n",
        "###End code here\n",
        "\n",
        "data_features_onehot_encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfAOG8SBu24E"
      },
      "source": [
        "*Q11: Optimize $C$ of a logistic regression model from the train set part of `data_features_onehot_encoding`:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bq2cZVNSu24F"
      },
      "outputs": [],
      "source": [
        "###Start code here\n",
        "\n",
        "###End code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po30zuMIXxii"
      },
      "source": [
        "*Q12: Fit a logistic regression model with the obtained optimal value $C$ on the train set:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jk0VAurlXw6L"
      },
      "outputs": [],
      "source": [
        "###Start code here\n",
        "\n",
        "###End code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vqh-I3zuu24L"
      },
      "source": [
        "*Q13: What is the accuracy on the test data in `data_features_onehot_encoding`?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeCkxuubu24M"
      },
      "outputs": [],
      "source": [
        "###Start code here\n",
        "\n",
        "###Start code here\n",
        "\n",
        "score_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "wRe1CLnUu24l"
      },
      "source": [
        "### **Feature importance**\n",
        "\n",
        "In scikit-learn, a fitted logistic regression model has the fitted modelparameter values stored in `.coef_[0]`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f-NBEUt4AjE"
      },
      "outputs": [],
      "source": [
        "#THe following code assumes \"model\" is the fitted logistic regression model\n",
        "print(model.coef_[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdPAICuc4NWT"
      },
      "source": [
        "This is one modelparameter for each feature (plus the interecept, which is not in `.coef_[0]`).\n",
        "\n",
        "Recall that in logistic regression a prediction is made by multiplying each fitted modelparameter with the corresponding feature value, summing them and then squeezing this sum between 0 and 1 with the sigmoid function.\n",
        "\n",
        "Since all features have values 0 or 1, the modelparameter values indicate the contribution (importance) of a feature to the prediction.\n",
        "\n",
        "The following code puts the feature names and modelparameter values in a new DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Mnte18-u242"
      },
      "outputs": [],
      "source": [
        "F_importances = []\n",
        "\n",
        "for feature_name, modelparameter in zip(data_features_onehot_encoding.columns,model.coef_[0]):\n",
        "    F_importances.append([feature_name,modelparameter])\n",
        "\n",
        "F_importances = pd.DataFrame(F_importances,columns=[\"feature_name\",\"importance\"])\n",
        "F_importances.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUmhfaF3JKuC"
      },
      "source": [
        "*Q14: What are the 20 features considered most important by the fitted logistic regression model?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHtatQhhJKuC"
      },
      "outputs": [],
      "source": [
        "#Start code here\n",
        "\n",
        "#End code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DxuK1cRbqmH"
      },
      "source": [
        "### **Feature selection**\n",
        "\n",
        "`sklearn.feature_selection.SelectFromModel` is a feature selection method that uses a trained model to identify and retain only the most important features, based on the model's learned feature importances or coefficients. This technique helps in reducing the dimensionality of the dataset, improving model performance, and preventing overfitting by focusing on the most relevant features.\n",
        "\n",
        "*Q15: Apply `sklearn.feature_selection.SelectFromModel` to return the 20 most important features from the previously fitted logistic regession model:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xkMq_YLJKuC"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "#Start code here\n",
        "\n",
        "\n",
        "selected_feature_names =\n",
        "\n",
        "#End code here\n",
        "\n",
        "print(\"Selected features:\", selected_feature_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wvxMJCoedEQ"
      },
      "source": [
        "*Q16: Are these the same features as were selected in Q15?*\n",
        "\n",
        "*Answer:*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyUdiF7lf9VD"
      },
      "source": [
        "*Q17: Optimize and fit a logistic regression on the train set that uses the selected 20 most important features only. What is the accuracy of this model in the test set?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRL4LsFgb7Fe"
      },
      "outputs": [],
      "source": [
        "#Start code here\n",
        "\n",
        "#End code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySbJPCIWhm8V"
      },
      "source": [
        "Recursive Feature Elimination (RFE) is a feature selection method that recursively removes the least important features based on the model's coefficients or feature importances until the desired number of features is reached. This iterative process helps in identifying the most relevant features for improving model performance and reducing complexity.\n",
        "\n",
        "*Q18: Use `sklearn.feature_selection.RFE` to select the 20 most important features (according to the RFE algorithm) from `data_features_onehot_encoding_train` for logistic regresion model with $C=0.001$. Set the `step` hyperparameter of RFE to 1. Are these same features selected as in Q15?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_CxZ8JGJKuC"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "#Start code here\n",
        "\n",
        "#End code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6fagnOGl9op"
      },
      "source": [
        "*Answer:*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUczEh6OJKuD"
      },
      "source": [
        "*[1] Dreos, R., Ambrosini, G., Perier, R. C., & Bucher, P. (2013). The Eukaryotic Promoter Database: expansion of EPDnew and new promoter analysis tools. Nucleic Acids Research, 41(D1), D50-D56.*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "core_promoter_prediction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
